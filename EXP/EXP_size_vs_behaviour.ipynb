{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#platform=input('Quale social vuoi utilizzare?')\nplatform='reddit'\nimport os\nGITHUB_USERNAME = 'jacoponudo'\nGITHUB_TOKEN = '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' \nrepository = 'jacoponudo/Size_effects'\nclone_url = f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/{repository}.git\"\n!git clone {clone_url}\nroot='/kaggle/working/'\nimport sys\nsys.path.append('/kaggle/working/Size_effects/EXP/EXP_package')\nfrom functions import *\n\nsys.path.append(module_path)\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nfrom scipy import stats\nimport random\nfrom scipy.stats import chi2\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.gofplots import qqplot\nimport pandas as pd\nimport tqdm \nimport seaborn as sns\n!pip install fastparquet\n!pip install gdown\n\n\nif platform=='gab':\n    # Scarica il dataset di Gab\n    import gdown\n\n    url='https://drive.google.com/uc?id=1CpsRAaBVv4hIoq713KQmWYyZOlMG2BpH'\n    output='gab.parquet'\n\n    gdown.download(url,output,quiet=False)\n    gab = pd.read_parquet('/kaggle/working/gab.parquet')\n\n    social=gab\n    social['user_id']=social['user']\nif platform=='reddit':\n    # Scarica il dataset di Reddit\n    import gdown\n\n    url='https://drive.google.com/uc?id=1QepHehlhqP-jtOcshFzajqxj1DpJtoj7'\n    output='reddit.parquet'\n\n    gdown.download(url,output,quiet=False)\n    reddit = pd.read_parquet('/kaggle/working/reddit.parquet')\n\n    social=reddit\n    social['created_at']=social['date']\nif platform=='facebook':\n    # Scarica i tre dataset che appartengono a Facebook\n    import gdown\n    url='https://drive.google.com/uc?id=1Y2lGWkcgo_IWHdWFh_Qcn0K74D_xQhvB'\n    output='facebook_news.csv'\n    gdown.download(url,output,quiet=False)\n\n    # Leggi i tre dataset con pandas\n    facebook_news = pd.read_csv('/kaggle/working/facebook_news.csv', dtype={'from_id': str})\n\n    facebook_news = facebook_news.copy()\n    facebook_news['topic'] = 'News'\n\n    facebook =facebook_news\n    facebook.reset_index(drop=True, inplace=True)\n\n    social=facebook\n    social['created_at'] = social['created_at'].apply(lambda x: x if ' ' in x else x + ' 00:00:00')\n\nsocial['created_at'] = pd.to_datetime(social['created_at'])","metadata":{"_uuid":"7d1282a2-28ec-4528-883e-bdae58203a3b","_cell_guid":"f5b37ff6-186c-4e15-ba05-8291f6333659","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-13T16:08:18.841632Z","iopub.execute_input":"2024-09-13T16:08:18.842076Z","iopub.status.idle":"2024-09-13T16:09:05.251445Z","shell.execute_reply.started":"2024-09-13T16:08:18.842034Z","shell.execute_reply":"2024-09-13T16:09:05.250234Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"fatal: destination path 'Size_effects' already exists and is not an empty directory.\nCollecting fastparquet\n  Downloading fastparquet-2024.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\nRequirement already satisfied: pandas>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from fastparquet) (2.2.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fastparquet) (1.26.4)\nCollecting cramjam>=2.3 (from fastparquet)\n  Downloading cramjam-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from fastparquet) (2024.6.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from fastparquet) (21.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2024.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->fastparquet) (3.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\nDownloading fastparquet-2024.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cramjam-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: cramjam, fastparquet\nSuccessfully installed cramjam-2.8.3 fastparquet-2024.5.0\nCollecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.15.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.7.4)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.2.0\n","output_type":"stream"},{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1QepHehlhqP-jtOcshFzajqxj1DpJtoj7\nFrom (redirected): https://drive.google.com/uc?id=1QepHehlhqP-jtOcshFzajqxj1DpJtoj7&confirm=t&uuid=202fecf4-e49f-473b-8e16-acace3f160eb\nTo: /kaggle/working/reddit.parquet\n100%|██████████| 334M/334M [00:07<00:00, 46.5MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"# Size vs interaction\n\nuser_counts = social.groupby(['post_id', 'user_id'])['user_id'].count().reset_index(name='user_count')\nuser_unique = social.groupby('post_id')['user_id'].nunique().reset_index(name='user_unique')\ndf=user_unique.merge(user_counts,on='post_id')\ndf=df.drop_duplicates(subset=['user_id','post_id'])\ndf.columns=['post_id','num_people','user_id','num_comments']\ndf=df[['num_people','num_comments']]\ndf.to_csv(platform+'_size_vs_interaction.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-12T14:40:44.174420Z","iopub.execute_input":"2024-09-12T14:40:44.174962Z","iopub.status.idle":"2024-09-12T14:41:03.147317Z","shell.execute_reply.started":"2024-09-12T14:40:44.174919Z","shell.execute_reply":"2024-09-12T14:41:03.146145Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Size vs inter arrival time \n\nsocial['created_at'] = pd.to_datetime(social['created_at'])\nsocial = social.sort_values(by=['user_id', 'post_id', 'created_at'])\nsocial['IAT'] = social.groupby(['user_id', 'post_id'])['created_at'].diff().dt.total_seconds()/60\n\nuser_unique = social.groupby('post_id')['user_id'].nunique().reset_index(name='size')\ndf=user_unique.merge(social,on='post_id')\ndf=df[['IAT','size']]\ndf.to_csv(platform+'_size_vs_iat.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Size vs inter arrival comment \n\nsocial['created_at'] = pd.to_datetime(social['created_at'])\nsocial = social.sort_values(by=['post_id', 'created_at', 'user_id'])\nsocial['user_shift'] = social.groupby(['post_id', 'user_id']).cumcount()\nsocial['comment_position'] = social.groupby('post_id').cumcount()\nsocial['next_comment_position'] = social.groupby(['post_id', 'user_id'])['comment_position'].shift(-1)\nsocial['IAC'] = social['next_comment_position'] - social['comment_position'] - 1\n\nuser_unique = social.groupby('post_id')['user_id'].nunique().reset_index(name='size')\ndf=user_unique.merge(social,on='post_id')\ndf=df[['IAC','size']]\ndf.to_csv(platform+'_size_vs_iac.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-12T14:38:02.471215Z","iopub.execute_input":"2024-09-12T14:38:02.471760Z","iopub.status.idle":"2024-09-12T14:38:44.365502Z","shell.execute_reply.started":"2024-09-12T14:38:02.471712Z","shell.execute_reply":"2024-09-12T14:38:44.364238Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Size vs interaction randomized\n\nsocial['user_id_shuffled'] = social['user_id'].sample(frac=1).reset_index(drop=True)\nuser_counts = social.groupby(['post_id', 'user_id_shuffled'])['user_id'].count().reset_index(name='user_count')\nuser_unique = social.groupby('post_id')['user_id_shuffled'].nunique().reset_index(name='user_unique')\ndf=user_unique.merge(user_counts,on='post_id')\ndf=df.drop_duplicates(subset=['user_id_shuffled','post_id'])\ndf.columns=['post_id','num_people','user_id_shuffled','num_comments']\n#df=df[['num_people','num_comments']]\ndf.to_csv(platform+'_random_size_vs_interaction.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-13T16:11:03.673063Z","iopub.execute_input":"2024-09-13T16:11:03.674214Z","iopub.status.idle":"2024-09-13T16:11:12.373690Z","shell.execute_reply.started":"2024-09-13T16:11:03.674163Z","shell.execute_reply":"2024-09-13T16:11:12.372493Z"},"trusted":true},"execution_count":8,"outputs":[]}]}