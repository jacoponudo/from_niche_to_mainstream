{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#platform=input('Quale social vuoi utilizzare?')\nplatform='facebook'\n!git clone https://github.com/jacoponudo/Size_effects.git\nroot='/kaggle/working/'\nimport sys\nmodule_path = root+'Size_effects/EXP'\nsys.path.append(module_path)\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nfrom scipy import stats\nimport random\nfrom scipy.stats import chi2\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.gofplots import qqplot\nimport pandas as pd\nfrom EXP_package.functions import *\nimport tqdm \nimport seaborn as sns\n!pip install fastparquet\n!pip install gdown\n\n\nif platform=='gab':\n    # Scarica il dataset di Gab\n    import gdown\n\n    url='https://drive.google.com/uc?id=1CpsRAaBVv4hIoq713KQmWYyZOlMG2BpH'\n    output='gab.parquet'\n\n    gdown.download(url,output,quiet=False)\n    gab = pd.read_parquet('/kaggle/working/gab.parquet')\n\n    social=gab\n    social['user_id']=social['user']\nif platform=='reddit':\n    # Scarica il dataset di Reddit\n    import gdown\n\n    url='https://drive.google.com/uc?id=1QepHehlhqP-jtOcshFzajqxj1DpJtoj7'\n    output='reddit.parquet'\n\n    gdown.download(url,output,quiet=False)\n    reddit = pd.read_parquet('/kaggle/working/reddit.parquet')\n\n    social=reddit\n    social['created_at']=social['date']\nif platform=='facebook':\n    # Scarica i tre dataset che appartengono a Facebook\n    import gdown\n    url='https://drive.google.com/uc?id=1Y2lGWkcgo_IWHdWFh_Qcn0K74D_xQhvB'\n    output='facebook_news.csv'\n    gdown.download(url,output,quiet=False)\n\n    # Leggi i tre dataset con pandas\n    facebook_news = pd.read_csv('/kaggle/working/facebook_news.csv', dtype={'from_id': str})\n\n    facebook_news = facebook_news.copy()\n    facebook_news['topic'] = 'News'\n\n    facebook =facebook_news\n    facebook.reset_index(drop=True, inplace=True)\n\n    social=facebook\n    social['created_at'] = social['created_at'].apply(lambda x: x if ' ' in x else x + ' 00:00:00')\n\nsocial['created_at'] = pd.to_datetime(social['created_at'])","metadata":{"_uuid":"7d1282a2-28ec-4528-883e-bdae58203a3b","_cell_guid":"f5b37ff6-186c-4e15-ba05-8291f6333659","collapsed":false,"jupyter":{"outputs_hidden":false},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Size vs interaction\n\nuser_counts = social.groupby(['post_id', 'user_id'])['user_id'].count().reset_index(name='user_count')\nuser_unique = social.groupby('post_id')['user_id'].nunique().reset_index(name='user_unique')\ndf=user_unique.merge(user_counts,on='post_id')\ndf=df.drop_duplicates(subset=['user_id','post_id'])\ndf.columns=['post_id','num_people','user_id','num_comments']\ndf=df[['num_people','num_comments']]\ndf.to_csv(platform+'_size_vs_interaction.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-12T14:40:44.174420Z","iopub.execute_input":"2024-09-12T14:40:44.174962Z","iopub.status.idle":"2024-09-12T14:41:03.147317Z","shell.execute_reply.started":"2024-09-12T14:40:44.174919Z","shell.execute_reply":"2024-09-12T14:41:03.146145Z"},"editable":false,"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Size vs inter arrival time \n\nsocial['created_at'] = pd.to_datetime(social['created_at'])\nsocial = social.sort_values(by=['user_id', 'post_id', 'created_at'])\nsocial['IAT'] = social.groupby(['user_id', 'post_id'])['created_at'].diff().dt.total_seconds()/60\n\nuser_unique = social.groupby('post_id')['user_id'].nunique().reset_index(name='size')\ndf=user_unique.merge(social,on='post_id')\ndf=df[['IAT','size']]\ndf.to_csv(platform+'_size_vs_iat.csv')\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Size vs inter arrival comment \n\nsocial['created_at'] = pd.to_datetime(social['created_at'])\nsocial = social.sort_values(by=['post_id', 'created_at', 'user_id'])\nsocial['user_shift'] = social.groupby(['post_id', 'user_id']).cumcount()\nsocial['comment_position'] = social.groupby('post_id').cumcount()\nsocial['next_comment_position'] = social.groupby(['post_id', 'user_id'])['comment_position'].shift(-1)\nsocial['IAC'] = social['next_comment_position'] - social['comment_position'] - 1\n\nuser_unique = social.groupby('post_id')['user_id'].nunique().reset_index(name='size')\ndf=user_unique.merge(social,on='post_id')\ndf=df[['IAC','size']]\ndf.to_csv(platform+'_size_vs_iac.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-12T14:38:02.471215Z","iopub.execute_input":"2024-09-12T14:38:02.471760Z","iopub.status.idle":"2024-09-12T14:38:44.365502Z","shell.execute_reply.started":"2024-09-12T14:38:02.471712Z","shell.execute_reply":"2024-09-12T14:38:44.364238Z"},"editable":false,"trusted":true},"execution_count":33,"outputs":[]}]}