{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import getpass\n\nplatform = input('Quale social vuoi utilizzare? ')\ntoken = getpass.getpass('Inserisci il tuo token di accesso: ')\nrepo_url = f'https://{token}:x-oauth-basic@github.com/jacoponudo/Size_effects.git'\n# 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\n!git clone {repo_url}","metadata":{"execution":{"iopub.status.busy":"2024-09-27T10:05:18.474270Z","iopub.execute_input":"2024-09-27T10:05:18.474934Z","iopub.status.idle":"2024-09-27T10:05:39.138924Z","shell.execute_reply.started":"2024-09-27T10:05:18.474856Z","shell.execute_reply":"2024-09-27T10:05:39.137321Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdin","text":"Quale social vuoi utilizzare?  facebook\nInserisci il tuo token di accesso:  ········································\n"},{"name":"stdout","text":"Cloning into 'Size_effects'...\nremote: Enumerating objects: 250, done.\u001b[K\nremote: Counting objects: 100% (143/143), done.\u001b[K\nremote: Compressing objects: 100% (112/112), done.\u001b[K\nremote: Total 250 (delta 58), reused 85 (delta 25), pack-reused 107 (from 1)\u001b[K\nReceiving objects: 100% (250/250), 94.45 MiB | 18.60 MiB/s, done.\nResolving deltas: 100% (73/73), done.\nUpdating files: 100% (55/55), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import getpass\n\nplatform = input('Quale social vuoi utilizzare? ')\ntoken = getpass.getpass('Inserisci il tuo token di accesso: ')\nrepo_url = f'https://{token}:x-oauth-basic@github.com/jacoponudo/Size_effects.git'\n# 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\n!git clone {repo_url}\nroot='/kaggle/working/'\nimport sys\nmodule_path = root+'Size_effects/EXP'\nsys.path.append(module_path)\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nfrom scipy import stats\nimport random\nfrom scipy.stats import chi2\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.gofplots import qqplot\nimport pandas as pd\nimport tqdm \nimport seaborn as sns\n!pip install fastparquet\n!pip install gdown\n\n\nif platform=='gab':\n    # Scarica il dataset di Gab\n    import gdown\n\n    url='https://drive.google.com/uc?id=1CpsRAaBVv4hIoq713KQmWYyZOlMG2BpH'\n    output='gab.parquet'\n\n    gdown.download(url,output,quiet=False)\n    gab = pd.read_parquet('/kaggle/working/gab.parquet')\n\n    social=gab\n    social['user_id']=social['user']\nif platform=='reddit':\n    # Scarica il dataset di Reddit\n    import gdown\n\n    url='https://drive.google.com/uc?id=1QepHehlhqP-jtOcshFzajqxj1DpJtoj7'\n    output='reddit.parquet'\n\n    gdown.download(url,output,quiet=False)\n    reddit = pd.read_parquet('/kaggle/working/reddit.parquet')\n\n    social=reddit\n    social['created_at']=social['date']\nif platform=='facebook':\n    # Scarica i tre dataset che appartengono a Facebook\n    import gdown\n    url='https://drive.google.com/uc?id=1Y2lGWkcgo_IWHdWFh_Qcn0K74D_xQhvB'\n    output='facebook_news.csv'\n    gdown.download(url,output,quiet=False)\n\n    # Leggi i tre dataset con pandas\n    facebook_news = pd.read_csv('/kaggle/working/facebook_news.csv', dtype={'from_id': str})\n\n    facebook_news = facebook_news.copy()\n    facebook_news['topic'] = 'News'\n\n    facebook =facebook_news\n    facebook.reset_index(drop=True, inplace=True)\n\n    social=facebook","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-27T11:14:55.330284Z","iopub.execute_input":"2024-09-27T11:14:55.330770Z","iopub.status.idle":"2024-09-27T11:17:03.766350Z","shell.execute_reply.started":"2024-09-27T11:14:55.330728Z","shell.execute_reply":"2024-09-27T11:17:03.764740Z"},"trusted":true},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdin","text":"Quale social vuoi utilizzare?  facebook\nInserisci il tuo token di accesso:  ········································\n"},{"name":"stdout","text":"fatal: destination path 'Size_effects' already exists and is not an empty directory.\nRequirement already satisfied: fastparquet in /opt/conda/lib/python3.10/site-packages (2024.5.0)\nRequirement already satisfied: pandas>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from fastparquet) (2.2.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fastparquet) (1.26.4)\nRequirement already satisfied: cramjam>=2.3 in /opt/conda/lib/python3.10/site-packages (from fastparquet) (2.8.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from fastparquet) (2024.6.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from fastparquet) (21.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2024.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->fastparquet) (3.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\nRequirement already satisfied: gdown in /opt/conda/lib/python3.10/site-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.15.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.8.30)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n","output_type":"stream"},{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1Y2lGWkcgo_IWHdWFh_Qcn0K74D_xQhvB\nFrom (redirected): https://drive.google.com/uc?id=1Y2lGWkcgo_IWHdWFh_Qcn0K74D_xQhvB&confirm=t&uuid=3625b017-bdc5-4881-9d60-7fcaede9f7eb\nTo: /kaggle/working/facebook_news.csv\n100%|██████████| 1.84G/1.84G [00:08<00:00, 229MB/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Test size effect vs platform effect ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nplatform='facebook'\nsocial=facebook\n# Filtra i dati e campiona 30.000 post_id\n#social = reddit#[facebook['post_id'].isin(facebook['post_id'].sample(50000))]\n\nif platform == 'reddit':\n    social = social.drop(columns=['date'])\n\n# Assicurati che 'created_at' sia in formato datetime\nif platform == 'facebook':\n    social['created_at'] = social['created_at'].apply(lambda x: x if ' ' in x else x + ' 00:00:00')\n\nsocial['created_at'] = pd.to_datetime(social['created_at'])\n\n# Calcola il numero di commenti per utente e post\ncomment_count = social.groupby(['post_id', 'user_id']).size().reset_index(name='comment_count')\n\n# Crea la variabile alpha\ncomment_count['alpha'] = comment_count['comment_count'] == 1\n\n# Trova l'anno del primo commento per ogni post_id\nfirst_comment_year = social.groupby('post_id')['created_at'].min().dt.year.reset_index(name='year')\n\n# Unisci i risultati\nrisultato = pd.merge(comment_count[['post_id', 'user_id', 'alpha','comment_count']], \n                     first_comment_year, \n                     on='post_id', \n                     how='left')\n\n# Conta gli utenti unici per ogni post_id\nuser_count = social.groupby('post_id')['user_id'].nunique().reset_index(name='size')\n\n# Unisci il conteggio degli utenti unici\nrisultato = pd.merge(risultato, user_count, on='post_id', how='left')\n\n# Mostra i risultati finali\nprint(risultato)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assume 'risultato' is your already created DataFrame\n\n# Define colors based on the platform\ncolors = {\n    'reddit': '#FF5700',\n    'voat': '#800080',\n    'facebook': '#3b5998',\n    'gab': '#00c853'\n}\n\n# Convert platform to uppercase (ensure 'platform' variable is defined)\nplatform_upper = platform.upper()\n\n# Define new bins and labels with steps of 200 after 1000\nbins = [0, 50, 100, 150, 200, 300, 400, 500, 700, 900, 1100]\nlabels = ['0-50', '51-100', '101-150', '151-200', '201-300', '301-400', \n          '401-500', '501-700', '701-900', '901-1100']\n\n# Create categories for the 'size' column\nrisultato['size_cat'] = pd.cut(risultato['size'], bins=bins, labels=labels)\n\n# Calculate the mean of alpha for each category and year\nmedia_alpha = risultato.groupby(['size_cat', 'year'])['alpha'].mean().reset_index()\n\n# Loop through the years and create a separate plot for each year\nyears = sorted(risultato['year'].unique())\nnum_years = len(years)\n\nfig, axes = plt.subplots(1, num_years, figsize=(5 * num_years, 6), sharey=True)\n\nfor i, year in enumerate(years):\n    year_data = media_alpha[media_alpha['year'] == year]\n    \n    # Assign color based on platform\n    color = colors.get(platform.lower(), '#000000')  # Default to black if platform not found\n    \n    # Create the plot for the current year\n    axes[i].plot(year_data['size_cat'], year_data['alpha'], marker='o', color=color)\n    \n    # Add title and labels\n    axes[i].set_title(f'Mean Alpha for {platform_upper} in {year}')\n    axes[i].set_xlabel('Size Categories', fontsize=5)  # Reduce x-axis label size\n    axes[i].set_ylabel('Mean Alpha')\n    axes[i].set_ylim(0.5, 1)  # Set y-axis limits\n    axes[i].set_xticks(year_data['size_cat'])\n    axes[i].grid()\n\n# Adjust layout\nplt.tight_layout()\n\n# Save the image\nfilename = f\"{platform}_year_len_size.jpg\"\nplt.savefig(filename)\n\n# Show the plot\nplt.show()\n\n# Close the figure\nplt.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}