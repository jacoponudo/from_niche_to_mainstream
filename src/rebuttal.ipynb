{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]/tmp/ipykernel_27051/2447249769.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  platform_data['platform']= platform\n",
      " 17%|█▋        | 1/6 [00:02<00:13,  2.63s/it]/tmp/ipykernel_27051/2447249769.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  platform_data['platform']= platform\n",
      " 33%|███▎      | 2/6 [00:24<00:55, 13.89s/it]/tmp/ipykernel_27051/2447249769.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  platform_data['platform']= platform\n",
      " 50%|█████     | 3/6 [00:28<00:28,  9.42s/it]/tmp/ipykernel_27051/2447249769.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  platform_data['platform']= platform\n",
      " 67%|██████▋   | 4/6 [00:50<00:28, 14.46s/it]/tmp/ipykernel_27051/2447249769.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  platform_data['platform']= platform\n",
      " 83%|████████▎ | 5/6 [03:11<01:00, 60.03s/it]/tmp/ipykernel_27051/2447249769.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  platform_data['platform']= platform\n",
      "100%|██████████| 6/6 [05:36<00:00, 56.14s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tools.to_read import *\n",
    "from tools.to_do import *\n",
    "from tools.to_plot import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Platform types and index categories\n",
    "platforms = ['reddit', 'usenet', 'voat', 'gab', 'facebook', 'twitter']\n",
    "\n",
    "# Create an empty list to store data for each platform\n",
    "all_data = []\n",
    "\n",
    "# Iterate over the different platform types and index types\n",
    "for platform in tqdm(platforms):\n",
    "    # Load and preprocess data for the platform\n",
    "    df = read_and_rename(platform, root)\n",
    "    df.dropna(subset=['user_id', 'post_id'], inplace=True)\n",
    "    \n",
    "    # Calcolare il numero di utenti per ogni post\n",
    "    user_count_per_post = df.groupby('post_id')['user_id'].nunique().reset_index()\n",
    "    user_count_per_post.columns = ['post_id', 'user_count']\n",
    "\n",
    "    # Aggiungere la colonna 'user_count' al DataFrame originale\n",
    "    df = df.merge(user_count_per_post, on='post_id', how='left')\n",
    "\n",
    "    # Identificare le coppie di utenti e post duplicati e rinominare 'reentry' in 'phi'\n",
    "    df['1-alpha'] = df.duplicated(subset=['user_id', 'post_id'], keep=False)\n",
    "\n",
    "    # Contare quante volte si ripete ciascuna coppia di 'user_id' e 'post_id'\n",
    "    df['K'] = df.groupby(['user_id', 'post_id'])['user_id'].transform('count')\n",
    "\n",
    "    # Spezzare 'user_count' in categorie (0, 30, 90, 150)\n",
    "    bin_start = 10\n",
    "    bin_end = 500\n",
    "\n",
    "    bins = np.logspace(np.log10(bin_start), np.log10(bin_end), num=13)\n",
    "    labels = [f'{int(bins[i]):,}-{int(bins[i+1]):,}' for i in range(len(bins)-1)]\n",
    "    df['user_count_binned'] = pd.cut(df['user_count'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Selezionare un campione casuale di 100.000 righe per piattaforma\n",
    "    platform_data = df[['user_count_binned', '1-alpha', 'K']]  # random_state per riproducibilità\n",
    "    platform_data['platform']= platform  \n",
    "    all_data.append(platform_data)\n",
    "\n",
    "\n",
    "# Concatenare tutti i dati in un unico DataFrame\n",
    "final_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Esportare i dati finali in un file CSV\n",
    "final_data.to_csv(root + 'src/output/model_crowd/k_vs_crowd.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... continue on r ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il pacchetto necessario\n",
    "library(dplyr)\n",
    "\n",
    "# Leggi i dati dal file CSV\n",
    "file_path <- \"/home/jacoponudo/Documents/from_niche_to_mainstream/src/output/model_crowd/k_vs_crowd.csv\"\n",
    "data <- read.csv(file_path)\n",
    "\n",
    "data$user_count_binned=fill_na(data$user_count_binned, \"0-10\")\n",
    "# Installare il pacchetto pscl se non è già installato\n",
    "if (!require(pscl)) {\n",
    "  install.packages(\"pscl\")\n",
    "}\n",
    "\n",
    "# Caricare il pacchetto pscl\n",
    "library(pscl)\n",
    "\n",
    "# Prepara la colonna Y\n",
    "data$Y = data$k - 1\n",
    "\n",
    "# Per ogni piattaforma, crea un modello ZIP e stampa il summary\n",
    "platforms <- unique(data$platform)\n",
    "data$user_count_binned <- factor(data$user_count_binned, \n",
    "                                 levels = c(\"0-30\", \"30-90\", \"90-150\", \"150+\"))  # Cambia questi livelli se necessario\n",
    "for (platform in platforms) {\n",
    "  platform_data <- data[data$platform == platform, ]\n",
    "  print(mean(platform_data$k))\n",
    "  \n",
    "  # Creare il modello ZIP per ogni piattaforma\n",
    "  model_zip <- zeroinfl(Y ~ user_count_binned, data = platform_data, dist = \"poisson\")\n",
    "  \n",
    "  # Stampa il riassunto del modello\n",
    "  cat(\"\\nSummary for platform:\", platform, \"\\n\")\n",
    "  print(summary(model_zip))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]/tmp/ipykernel_3237/4222178043.py:35: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df['week'] = df['timestamp'].dt.to_period('W')  # Extract the week from timestamp\n",
      " 17%|█▋        | 1/6 [00:02<00:13,  2.79s/it]/tmp/ipykernel_3237/4222178043.py:35: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df['week'] = df['timestamp'].dt.to_period('W')  # Extract the week from timestamp\n",
      " 33%|███▎      | 2/6 [00:27<01:02, 15.63s/it]/tmp/ipykernel_3237/4222178043.py:35: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df['week'] = df['timestamp'].dt.to_period('W')  # Extract the week from timestamp\n",
      " 50%|█████     | 3/6 [00:30<00:30, 10.11s/it]/tmp/ipykernel_3237/4222178043.py:35: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df['week'] = df['timestamp'].dt.to_period('W')  # Extract the week from timestamp\n",
      " 67%|██████▋   | 4/6 [00:45<00:23, 11.95s/it]/tmp/ipykernel_3237/4222178043.py:35: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df['week'] = df['timestamp'].dt.to_period('W')  # Extract the week from timestamp\n",
      "100%|██████████| 6/6 [05:10<00:00, 51.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m         all_data.append(merged_data)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Concatenate all platform data into one DataFrame\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m all_data_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m all_data_df.to_csv(root + \u001b[33m'\u001b[39m\u001b[33msrc/output/model2/reentry_vs_outreach.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/from_niche_to_mainstream/.venv/lib/python3.12/site-packages/pandas/core/reshape/concat.py:395\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/from_niche_to_mainstream/.venv/lib/python3.12/site-packages/pandas/core/reshape/concat.py:684\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    680\u001b[39m             indexers[ax] = obj_labels.get_indexer(new_labels)\n\u001b[32m    682\u001b[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m new_data = \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[32m    688\u001b[39m     new_data._consolidate_inplace()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/from_niche_to_mainstream/.venv/lib/python3.12/site-packages/pandas/core/internals/concat.py:189\u001b[39m, in \u001b[36mconcatenate_managers\u001b[39m\u001b[34m(mgrs_indexers, axes, concat_axis, copy)\u001b[39m\n\u001b[32m    187\u001b[39m     fastpath = blk.values.dtype == values.dtype\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     values = \u001b[43m_concatenate_join_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m     fastpath = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fastpath:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/from_niche_to_mainstream/.venv/lib/python3.12/site-packages/pandas/core/internals/concat.py:486\u001b[39m, in \u001b[36m_concatenate_join_units\u001b[39m\u001b[34m(join_units, copy)\u001b[39m\n\u001b[32m    483\u001b[39m     concat_values = ensure_block_shape(concat_values, \u001b[32m2\u001b[39m)\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m     concat_values = \u001b[43mconcat_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m empty_dtype != empty_dtype_future:\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m empty_dtype == concat_values.dtype:\n\u001b[32m    490\u001b[39m         \u001b[38;5;66;03m# GH#39122, GH#40893\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/from_niche_to_mainstream/.venv/lib/python3.12/site-packages/pandas/core/dtypes/concat.py:126\u001b[39m, in \u001b[36mconcat_compat\u001b[39m\u001b[34m(to_concat, axis, ea_compat_axis)\u001b[39m\n\u001b[32m    115\u001b[39m         warnings.warn(\n\u001b[32m    116\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe behavior of array concatenation with empty entries is \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    117\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdeprecated. In a future version, this will no longer exclude \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m             stacklevel=find_stack_level(),\n\u001b[32m    123\u001b[39m         )\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     to_concat = [\u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m to_concat]\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(to_concat[\u001b[32m0\u001b[39m], np.ndarray):\n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# i.e. isinstance(to_concat[0], ExtensionArray)\u001b[39;00m\n\u001b[32m    130\u001b[39m     to_concat_eas = cast(\u001b[33m\"\u001b[39m\u001b[33mSequence[ExtensionArray]\u001b[39m\u001b[33m\"\u001b[39m, to_concat)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/from_niche_to_mainstream/.venv/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:179\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m values\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np.ndarray):\n\u001b[32m    178\u001b[39m     \u001b[38;5;66;03m# i.e. ExtensionArray\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     values = \u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    182\u001b[39m     values = _astype_nansafe(values, dtype, copy=copy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/from_niche_to_mainstream/.venv/lib/python3.12/site-packages/pandas/core/arrays/datetimes.py:739\u001b[39m, in \u001b[36mDatetimeArray.astype\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, PeriodDtype):\n\u001b[32m    738\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_period(freq=dtype.freq)\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdtl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDatetimeLikeArrayMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/from_niche_to_mainstream/.venv/lib/python3.12/site-packages/pandas/core/arrays/datetimelike.py:460\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin.astype\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m# *much* faster than self._box_values\u001b[39;00m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m#  for e.g. test_get_loc_tuple_monotonic_above_size_cutoff\u001b[39;00m\n\u001b[32m    459\u001b[39m     i8data = \u001b[38;5;28mself\u001b[39m.asi8\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     converted = \u001b[43mints_to_pydatetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi8data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtz\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtimestamp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreso\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_creso\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m converted\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype.kind == \u001b[33m\"\u001b[39m\u001b[33mm\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from tools.to_read import *\n",
    "from tools.to_do import *\n",
    "from tools.to_plot import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Platform types and index categories\n",
    "platforms = ['reddit', 'usenet', 'voat', 'gab', 'facebook', 'twitter']\n",
    "\n",
    "# Parameters for data filtering and processing\n",
    "ignore_under = 50  # Minimum outreach threshold to avoid U-shaped trends\n",
    "time_window = 12  # Time window for smoothing the time series (weeks)\n",
    "correction = 10  # Maximum value of interaction count for corrections\n",
    "\n",
    "# Create an empty list to store data for each platform\n",
    "all_data = []\n",
    "\n",
    "# Iterate over the different platform types and index types\n",
    "for platform in tqdm(platforms):\n",
    "        # Load and preprocess data for the platform\n",
    "        df = read_and_rename(platform, root)\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])  # Convert timestamp to datetime\n",
    "        df['week'] = df['timestamp'].dt.to_period('W')  # Extract the week from timestamp\n",
    "\n",
    "        # Group by 'page_id' and 'week' to count unique users\n",
    "        weekly_unique_users = df.groupby(['page_id', 'week'])['user_id'].nunique().reset_index()\n",
    "        weekly_unique_users.rename(columns={'user_id': 'unique_users_count'}, inplace=True)\n",
    "        weekly_unique_users = weekly_unique_users.sort_values(by=['page_id', 'week'])\n",
    "        \n",
    "        # Apply a moving average for smoothing\n",
    "        weekly_unique_users['smoothed_users_count'] = (\n",
    "            weekly_unique_users.groupby('page_id')['unique_users_count']\n",
    "            .rolling(window=time_window, min_periods=1)\n",
    "            .mean().reset_index(level=0, drop=True)\n",
    "        )\n",
    "        # Create a new 'reentry' column to flag duplicates for each user_id and post_id combination\n",
    "        df['reentry'] = df.duplicated(subset=['user_id', 'post_id'], keep=False).astype(int)\n",
    "\n",
    "        # Now drop duplicates based on 'user_id' and 'post_id'\n",
    "        df_no_duplicates = df.drop_duplicates(subset=['user_id', 'post_id'])\n",
    "        merged_data = pd.merge(weekly_unique_users, df_no_duplicates, on=['page_id', 'week'], how='right')\n",
    "        X = (merged_data[['smoothed_users_count']])\n",
    "        y = merged_data['reentry']\n",
    "\n",
    "        # Aggiungi un termine di intercetta per il modello\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        model = sm.Logit(y, X)\n",
    "\n",
    "        # Allenare il modello\n",
    "        result_model = model.fit()\n",
    "\n",
    "        # Mostra il summary del modello\n",
    "        print(f\"Platform: {platform}\")\n",
    "        print(result_model.summary())\n",
    "\n",
    "\n",
    "\n",
    "        # Aggiungi una colonna 'platform' per ogni piattaforma\n",
    "        merged_data['platform'] = platform\n",
    "        \n",
    "        # Aggiungi i dati della piattaforma corrente alla lista\n",
    "        all_data.append(merged_data)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Numero di righe da prendere da ogni data\n",
    "num_rows = 1220136\n",
    "\n",
    "# Lista per memorizzare i campioni\n",
    "sampled_data = []\n",
    "\n",
    "for data in all_data:\n",
    "    # Se il dataset ha più righe del numero desiderato, estrai un campione casuale\n",
    "    if len(data) > num_rows:\n",
    "        sampled_data.append(data.sample(n=num_rows, random_state=42))\n",
    "    else:\n",
    "        # Altrimenti, prendi tutte le righe\n",
    "        sampled_data.append(data)\n",
    "\n",
    "# Numero di righe da prendere da ogni data\n",
    "num_rows = 1220136\n",
    "\n",
    "# Lista per memorizzare i campioni\n",
    "sampled_data = []\n",
    "\n",
    "for data in all_data:\n",
    "    # Se il dataset ha più righe del numero desiderato, estrai un campione casuale\n",
    "    if len(data) > num_rows:\n",
    "        sampled_data.append(data.sample(n=num_rows, random_state=42))\n",
    "    else:\n",
    "        # Altrimenti, prendi tutte le righe\n",
    "        sampled_data.append(data)\n",
    "\n",
    "# Concatenare tutti i campioni\n",
    "final_data = pd.concat(sampled_data, ignore_index=True)\n",
    "\n",
    "final_data.to_csv(root + 'src/output/model_2/reentry_vs_outreach.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "continue on model2.r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il pacchetto necessario\n",
    "library(dplyr)\n",
    "\n",
    "# Leggi i dati dal file CSV\n",
    "file_path <- \"/home/jacoponudo/Documents/from_niche_to_mainstream/src/output/model_outreach/reentry_vs_outreach.csv\"\n",
    "data <- read.csv(file_path)\n",
    "\n",
    "# Crea un campione casuale del 10% dei dati (se necessario)\n",
    "set.seed(42)  # Imposta il seme per la riproducibilità\n",
    "sampled_data <- sample_frac(data, 1)  # Rimuovi questa linea se vuoi usare l'intero dataset\n",
    "\n",
    "# Visualizza le prime righe del campione per verificarne il contenuto\n",
    "head(sampled_data)\n",
    "\n",
    "# Assicurati che 'reentry' sia una variabile bina\n",
    "sampled_data$reentry <- as.factor(sampled_data$reentry)\n",
    "\n",
    "# Crea il modello di regressione logistica\n",
    "log_model <- glm(reentry ~ platform *smoothed_users_count, \n",
    "                 data = sampled_data, \n",
    "                 family = binomial())\n",
    "\n",
    "# Sommario del modello\n",
    "summary(log_model)\n",
    "library(ggplot2)\n",
    "\n",
    "# Calcola gli odds ratio\n",
    "odds_ratios <- exp(coef(log_model))\n",
    "\n",
    "# Crea un data frame per la visualizzazione\n",
    "odds_ratios_df <- data.frame(\n",
    "  Variable = names(odds_ratios),\n",
    "  Odds_Ratio = odds_ratios\n",
    ")\n",
    "\n",
    "# Plot dei coefficienti (Odds Ratio)\n",
    "ggplot(odds_ratios_df, aes(x = reorder(Variable, Odds_Ratio), y = Odds_Ratio)) +\n",
    "  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n",
    "  coord_flip() +  # Ruota il grafico per visualizzare meglio i nomi delle variabili\n",
    "  labs(title = \"Odds Ratios dei Coefficienti del Modello\",\n",
    "       x = \"Variabile\",\n",
    "       y = \"Odds Ratio\") +\n",
    "  theme_minimal()\n",
    "\n",
    "\n",
    "# Suddividi il dataset per piattaforma\n",
    "platforms <- unique(sampled_data$platform)\n",
    "\n",
    "# Crea una lista per memorizzare i risultati\n",
    "results_list <- list()\n",
    "\n",
    "# Applica il modello di regressione logistica per ogni piattaforma\n",
    "for (platform in platforms) {\n",
    "  # Filtro il dataset per la piattaforma corrente\n",
    "  platform_data <- sampled_data[sampled_data$platform == platform, ]\n",
    "  \n",
    "  # Crea il modello di regressione logistica\n",
    "  log_model <- glm(reentry ~ smoothed_users_count, \n",
    "                   data = platform_data, \n",
    "                   family = binomial())\n",
    "  \n",
    "  # Estrai i risultati del modello\n",
    "  model_summary <- summary(log_model)\n",
    "  coefficients <- model_summary$coefficients\n",
    "  p_values <- coefficients[, 4]  # Estrai i p-value\n",
    "  \n",
    "  # Estrai altre informazioni utili\n",
    "  deviance <- log_model$deviance\n",
    "  aic <- log_model$aic\n",
    "  z_values <- coefficients[, 3]  # Z-values\n",
    "  \n",
    "  # Salva i risultati in un data frame\n",
    "  results <- data.frame(\n",
    "    platform = platform,\n",
    "    coefficient = coefficients[, 1],\n",
    "    std_error = coefficients[, 2],\n",
    "    z_value = z_values,\n",
    "    p_value = p_values,\n",
    "    deviance = deviance,\n",
    "    aic = aic\n",
    "  )\n",
    "  \n",
    "  # Aggiungi i risultati alla lista\n",
    "  results_list[[platform]] <- results\n",
    "}\n",
    "\n",
    "# Combina tutti i risultati in un unico data frame\n",
    "final_results <- do.call(rbind, results_list)\n",
    "\n",
    "# Visualizza la tabella finale con tutte le informazioni\n",
    "print(final_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
