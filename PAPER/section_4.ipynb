{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.to_read import *\n",
    "from tools.to_plot import *\n",
    "from tools.to_do import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "platform='facebook'\n",
    "root = '/home/jacoponudo/Documents/Size_effects/' \n",
    "data = pd.read_parquet(root + 'DATA/' + platform + '/' + platform + '_raw_data.parquet')\n",
    "data.rename(columns={\n",
    "    'topic': 'page_id',\n",
    "    'from_id': 'user_id',  # This remains the same\n",
    "    'post_id': 'post_id',  # This remains the same\n",
    "    'created_time': 'timestamp'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.to_read import *\n",
    "from tools.to_plot import *\n",
    "from tools.to_do import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.image as mpimg\n",
    "platform='twitter'\n",
    "root = '/home/jacoponudo/Documents/Size_effects/' \n",
    "data = pd.read_parquet(root + 'DATA/' + platform + '/' + platform + '_raw_data.parquet')\n",
    "data.rename(columns={\n",
    "    'topic': 'page_id',\n",
    "    'author_id': 'user_id',  # This remains the same\n",
    "    'post_id': 'post_id',  # This remains the same\n",
    "    'created_at': 'timestamp'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climatechange</td>\n",
       "      <td>t2_6l4z3</td>\n",
       "      <td>7ndth3</td>\n",
       "      <td>2018-01-01 06:49:06+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climatechange</td>\n",
       "      <td>t2_wiu0</td>\n",
       "      <td>7ne93z</td>\n",
       "      <td>2018-01-01 09:59:09+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>climatechange</td>\n",
       "      <td>t2_mwmugwc</td>\n",
       "      <td>7nf0fb</td>\n",
       "      <td>2018-01-01 12:54:33+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>climatechange</td>\n",
       "      <td>t2_eq6o6</td>\n",
       "      <td>7ne93z</td>\n",
       "      <td>2018-01-01 13:58:32+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>climatechange</td>\n",
       "      <td>t2_51cb9</td>\n",
       "      <td>7ne93z</td>\n",
       "      <td>2018-01-01 14:41:15+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853618</th>\n",
       "      <td>vaccines</td>\n",
       "      <td>t2_cu1l3alr</td>\n",
       "      <td>o5l0n8</td>\n",
       "      <td>2022-11-02 04:24:47+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853619</th>\n",
       "      <td>vaccines</td>\n",
       "      <td>t2_6l4z3</td>\n",
       "      <td>o5l0n8</td>\n",
       "      <td>2022-11-06 03:12:37+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853620</th>\n",
       "      <td>vaccines</td>\n",
       "      <td>t2_qsub24zn</td>\n",
       "      <td>o5l0n8</td>\n",
       "      <td>2022-11-06 03:17:14+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853621</th>\n",
       "      <td>vaccines</td>\n",
       "      <td>t2_2bbv0vl9</td>\n",
       "      <td>o5l0n8</td>\n",
       "      <td>2022-11-06 15:03:29+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853622</th>\n",
       "      <td>vaccines</td>\n",
       "      <td>t2_qsub24zn</td>\n",
       "      <td>o5l0n8</td>\n",
       "      <td>2022-11-06 15:47:14+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1853623 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               page_id      user_id post_id                 timestamp\n",
       "0        climatechange     t2_6l4z3  7ndth3 2018-01-01 06:49:06+00:00\n",
       "1        climatechange      t2_wiu0  7ne93z 2018-01-01 09:59:09+00:00\n",
       "2        climatechange   t2_mwmugwc  7nf0fb 2018-01-01 12:54:33+00:00\n",
       "3        climatechange     t2_eq6o6  7ne93z 2018-01-01 13:58:32+00:00\n",
       "4        climatechange     t2_51cb9  7ne93z 2018-01-01 14:41:15+00:00\n",
       "...                ...          ...     ...                       ...\n",
       "1853618       vaccines  t2_cu1l3alr  o5l0n8 2022-11-02 04:24:47+00:00\n",
       "1853619       vaccines     t2_6l4z3  o5l0n8 2022-11-06 03:12:37+00:00\n",
       "1853620       vaccines  t2_qsub24zn  o5l0n8 2022-11-06 03:17:14+00:00\n",
       "1853621       vaccines  t2_2bbv0vl9  o5l0n8 2022-11-06 15:03:29+00:00\n",
       "1853622       vaccines  t2_qsub24zn  o5l0n8 2022-11-06 15:47:14+00:00\n",
       "\n",
       "[1853623 rows x 4 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "root = '/home/jacoponudo/Documents/Size_effects/'\n",
    "platform='reddit'\n",
    "df = pd.read_parquet(root + 'DATA/' + platform + '/' + platform + '_raw_data.parquet',columns=['topic','user_id','post_id','date'])\n",
    "\n",
    "df.rename(columns={\n",
    "    'topic': 'page_id',\n",
    "    'user_id': 'user_id',  # This remains the same\n",
    "    'post_id': 'post_id',  # This remains the same\n",
    "    'date': 'timestamp'\n",
    "}, inplace=True)\n",
    "data=df\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Voat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.to_read import *\n",
    "from tools.to_plot import *\n",
    "from tools.to_do import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.image as mpimg\n",
    "root = '/home/jacoponudo/Documents/Size_effects/'\n",
    "platform='voat'\n",
    "df = pd.read_parquet(root + 'DATA/' + platform + '/' + platform + '_raw_data.parquet',columns=['topic','user','root_submission','created_at'])\n",
    "\n",
    "df.rename(columns={\n",
    "    'topic': 'page_id',\n",
    "    'user': 'user_id',  # This remains the same\n",
    "    'root_submission': 'post_id',  # This remains the same\n",
    "    'created_at': 'timestamp'\n",
    "}, inplace=True)\n",
    "data=df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          page_id        week  unique_users_count  smoothed_users_count\n",
      "0  Climate Change  2019-12-31                   1                  1.00\n",
      "1  Climate Change  2020-01-01                  10                  5.50\n",
      "2  Climate Change  2020-01-02                  13                  8.00\n",
      "3  Climate Change  2020-01-03                   5                  7.25\n",
      "4  Climate Change  2020-01-04                   7                  7.20\n"
     ]
    }
   ],
   "source": [
    "df=data\n",
    "# Assicurati che la colonna 'timestamp' sia di tipo datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Aggiungi una colonna 'week' che rappresenta la settimana dell'anno\n",
    "df['week'] = df['timestamp'].dt.to_period('d')\n",
    "\n",
    "\n",
    "# Raggruppa per 'page_id', 'week' e calcola il numero di utenti univoci per ogni combinazione\n",
    "weekly_unique_users = df.groupby(['page_id', 'week'])['user_id'].nunique().reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# Rinominare la colonna per maggiore chiarezza\n",
    "weekly_unique_users.rename(columns={'user_id': 'unique_users_count'}, inplace=True)\n",
    "\n",
    "# Ordinare per 'page_id' e 'week' (assicurarsi che i dati siano in ordine)\n",
    "weekly_unique_users = weekly_unique_users.sort_values(by=['page_id', 'week'])\n",
    "\n",
    "# Aggiungere la media mobile a 3 settimane (o a una finestra che preferisci)\n",
    "weekly_unique_users['smoothed_users_count'] = weekly_unique_users.groupby('page_id')['unique_users_count'].rolling(window=30, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Mostrare i primi risultati\n",
    "print(weekly_unique_users.head())\n",
    "\n",
    "weekly_unique_users.to_csv(\"/home/jacoponudo/Documents/Size_effects/PAPER/output/4_section/ts_outreach.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbosity - Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14007/677518029.py:10: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  posts['week'] = posts['timestamp'].dt.to_period('d')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from tools.to_do import *\n",
    "\n",
    "# Raggruppiamo per 'post_id' e 'page_id' e otteniamo il timestamp minimo per ogni gruppo\n",
    "posts = data.groupby(['post_id', 'page_id'])['timestamp'].min().reset_index()\n",
    "\n",
    "# Convertiamo 'timestamp' in formato datetime\n",
    "posts['timestamp'] = pd.to_datetime(posts['timestamp'])\n",
    "\n",
    "# Creiamo una colonna 'week' che rappresenta la settimana del 'timestamp'\n",
    "posts['week'] = posts['timestamp'].dt.to_period('d')\n",
    "\n",
    "# Raggruppiamo per 'user_id' e 'post_id' per calcolare il numero di commenti per ogni post\n",
    "comments = data.groupby(['user_id', 'post_id']).size().reset_index(name='comment_count')\n",
    "\n",
    "# Impostiamo un limite per i commenti (se ci sono più di 5 commenti, li limitamo a 5)\n",
    "comments['comment_count'] = comments['comment_count'].apply(lambda x: 5 if x > 5 else x)\n",
    "\n",
    "# Calcoliamo la distribuzione di probabilità dei commenti per ogni post\n",
    "prob_dist = comments.groupby(['post_id'])['comment_count'].value_counts(normalize=True)\n",
    "\n",
    "# Calcoliamo il parametro di localizzazione per ogni post_id\n",
    "localization_results = prob_dist.groupby(['post_id']).apply(lambda x: calculate_localization_parameter(x.values)).reset_index(name='localization_parameter')\n",
    "\n",
    "# Aggiungiamo la colonna 'localization_parameter' al DataFrame 'posts' tramite un merge su 'post_id'\n",
    "posts = posts.merge(localization_results[['post_id', 'localization_parameter']], on='post_id', how='left')\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbosity - Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>week</th>\n",
       "      <th>is_last_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Climate Change</td>\n",
       "      <td>1.253473771413e+18</td>\n",
       "      <td>2020-04-23</td>\n",
       "      <td>0.367347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Climate Change</td>\n",
       "      <td>1.253473771413e+18</td>\n",
       "      <td>2020-04-24</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Climate Change</td>\n",
       "      <td>1.253473771413e+18</td>\n",
       "      <td>2020-04-25</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Climate Change</td>\n",
       "      <td>1.253473771413e+18</td>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Climate Change</td>\n",
       "      <td>1.253473771413e+18</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147975</th>\n",
       "      <td>Vaccines</td>\n",
       "      <td>999379623245963264</td>\n",
       "      <td>2020-10-19</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147976</th>\n",
       "      <td>Vaccines</td>\n",
       "      <td>999379623245963264</td>\n",
       "      <td>2021-02-25</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147977</th>\n",
       "      <td>Vaccines</td>\n",
       "      <td>999379623245963264</td>\n",
       "      <td>2021-02-26</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147978</th>\n",
       "      <td>Vaccines</td>\n",
       "      <td>999379623245963264</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147979</th>\n",
       "      <td>Vaccines</td>\n",
       "      <td>999379623245963264</td>\n",
       "      <td>2022-01-27</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1147980 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                page_id             post_id        week  is_last_comment\n",
       "0        Climate Change  1.253473771413e+18  2020-04-23         0.367347\n",
       "1        Climate Change  1.253473771413e+18  2020-04-24         0.800000\n",
       "2        Climate Change  1.253473771413e+18  2020-04-25         1.000000\n",
       "3        Climate Change  1.253473771413e+18  2020-04-27         1.000000\n",
       "4        Climate Change  1.253473771413e+18  2020-05-01         0.000000\n",
       "...                 ...                 ...         ...              ...\n",
       "1147975        Vaccines  999379623245963264  2020-10-19         1.000000\n",
       "1147976        Vaccines  999379623245963264  2021-02-25         0.857143\n",
       "1147977        Vaccines  999379623245963264  2021-02-26         1.000000\n",
       "1147978        Vaccines  999379623245963264  2021-02-28         1.000000\n",
       "1147979        Vaccines  999379623245963264  2022-01-27         1.000000\n",
       "\n",
       "[1147980 rows x 4 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from tools.to_do import *\n",
    "\n",
    "# Raggruppiamo per 'post_id' e 'page_id' e otteniamo il timestamp minimo per ogni gruppo\n",
    "posts = data.groupby(['post_id', 'page_id'])['timestamp'].min().reset_index()\n",
    "\n",
    "# Convertiamo 'timestamp' in formato datetime\n",
    "posts['timestamp'] = pd.to_datetime(posts['timestamp'])\n",
    "\n",
    "# Creiamo una colonna 'week' che rappresenta la settimana del 'timestamp'\n",
    "posts['week'] = posts['timestamp'].dt.to_period('d')\n",
    "\n",
    "# Raggruppiamo per 'user_id' e 'post_id' per calcolare il numero di commenti per ogni post\n",
    "# Grouping by 'user_id' and 'post_id' to count the comments\n",
    "# Sorting comments by user_id and post_id\n",
    "data = data.sort_values(by=['user_id', 'post_id', 'timestamp'])\n",
    "\n",
    "# Adding comment count for each user and post\n",
    "data = data.merge(data.groupby(['user_id', 'post_id']).size().reset_index(name='comment_count')[['user_id', 'post_id', 'comment_count']], on=['user_id', 'post_id'], how='left')\n",
    "\n",
    "# Marking the last comment for each user and post\n",
    "data['is_last_comment'] = data.groupby(['user_id', 'post_id'])['timestamp'].transform('last') == data['timestamp']\n",
    "posts = data.groupby(['page_id', 'post_id', 'week'])['is_last_comment'].mean().reset_index()\n",
    "posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                page_id        week  unique_users_count  smoothed_users_count  \\\n",
      "0        Climate Change  2020-04-23                1145            357.966667   \n",
      "1        Climate Change  2020-04-24                 203            344.800000   \n",
      "2        Climate Change  2020-04-25                 129            345.800000   \n",
      "3        Climate Change  2020-04-27                 199            368.566667   \n",
      "4        Climate Change  2020-05-01                 333            375.533333   \n",
      "...                 ...         ...                 ...                   ...   \n",
      "1147975        Vaccines  2020-10-19                1558           2355.033333   \n",
      "1147976        Vaccines  2021-02-25                1311          68194.533333   \n",
      "1147977        Vaccines  2021-02-26                1230          67751.233333   \n",
      "1147978        Vaccines  2021-02-28                 932          67517.033333   \n",
      "1147979        Vaccines  2022-01-27                 173            256.400000   \n",
      "\n",
      "                    post_id  is_last_comment  \n",
      "0        1.253473771413e+18         0.367347  \n",
      "1        1.253473771413e+18         0.800000  \n",
      "2        1.253473771413e+18         1.000000  \n",
      "3        1.253473771413e+18         1.000000  \n",
      "4        1.253473771413e+18         0.000000  \n",
      "...                     ...              ...  \n",
      "1147975  999379623245963264         1.000000  \n",
      "1147976  999379623245963264         0.857143  \n",
      "1147977  999379623245963264         1.000000  \n",
      "1147978  999379623245963264         1.000000  \n",
      "1147979  999379623245963264         1.000000  \n",
      "\n",
      "[1147980 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(weekly_unique_users, posts, on=['page_id',  'week'], how='right')\n",
    "\n",
    "# 'how' può essere 'inner', 'left', 'right' o 'outer' a seconda di come vuoi che vengano gestiti i valori non corrispondenti\n",
    "\n",
    "# Mostriamo il risultato del merge\n",
    "print(merged_df)\n",
    "merged_df.to_csv(root + f'PAPER/output/4_section/5_size_effect_{platform}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'localization_parameter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'localization_parameter'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m valid_bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(valid_bins, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m bin_: bin_\u001b[38;5;241m.\u001b[39mleft)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Calcolo del valore medio di 'localization_parameter' per ogni bin valido\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m mean_values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     24\u001b[0m     merged_df[merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinned\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m bin_][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalization_parameter\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m bin_ \u001b[38;5;129;01min\u001b[39;00m valid_bins\n\u001b[1;32m     26\u001b[0m ]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Creazione del line plot\u001b[39;00m\n\u001b[1;32m     29\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "Cell \u001b[0;32mIn[121], line 24\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m valid_bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(valid_bins, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m bin_: bin_\u001b[38;5;241m.\u001b[39mleft)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Calcolo del valore medio di 'localization_parameter' per ogni bin valido\u001b[39;00m\n\u001b[1;32m     23\u001b[0m mean_values \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mmerged_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmerged_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinned\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbin_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocalization_parameter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m bin_ \u001b[38;5;129;01min\u001b[39;00m valid_bins\n\u001b[1;32m     26\u001b[0m ]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Creazione del line plot\u001b[39;00m\n\u001b[1;32m     29\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'localization_parameter'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creazione dei bins logaritmici\n",
    "bin_start = 20\n",
    "bin_end =1000\n",
    "\n",
    "# Calcolare i limiti dei bins utilizzando logaritmi (log base 10)\n",
    "bins = np.logspace(np.log10(bin_start), np.log10(bin_end), num=10)\n",
    "\n",
    "# Aggiunta di una colonna per il bin in cui si trova ogni valore\n",
    "merged_df['binned'] = pd.cut(merged_df['smoothed_users_count'], bins, right=False)\n",
    "\n",
    "# Filtriamo i bins che hanno meno di 100 osservazioni\n",
    "binned_counts = merged_df['binned'].value_counts()\n",
    "valid_bins = binned_counts[binned_counts >= 100].index\n",
    "\n",
    "# Ordiniamo i bins in base al limite sinistro\n",
    "valid_bins = sorted(valid_bins, key=lambda bin_: bin_.left)\n",
    "\n",
    "# Calcolo del valore medio di 'localization_parameter' per ogni bin valido\n",
    "mean_values = [\n",
    "    merged_df[merged_df['binned'] == bin_]['localization_parameter'].mean()\n",
    "    for bin_ in valid_bins\n",
    "]\n",
    "\n",
    "# Creazione del line plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Disegno del grafico a linee\n",
    "plt.plot(np.arange(len(valid_bins)), mean_values, marker='o', linestyle='-',color=palette[platform])\n",
    "\n",
    "# Etichette per i bins\n",
    "bin_labels = [f'{int(bin_.left)} - {int(bin_.right)}' for bin_ in valid_bins]\n",
    "plt.xticks(np.arange(len(valid_bins)), bin_labels, rotation=45)\n",
    "\n",
    "# Aggiunta delle etichette agli assi\n",
    "plt.xlabel('Page outreach', fontsize=30)\n",
    "plt.ylabel('Localization', fontsize=30)\n",
    "plt.title(str(platform.capitalize()), fontsize=35)\n",
    "# Mostra il grafico\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{root}PAPER/output/4_section/5_size_effect_{platform}.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
